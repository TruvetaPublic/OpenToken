{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d416c681",
   "metadata": {},
   "source": [
    "# Privacy-Preserving Record Linkage (PPRL) Demonstration\n",
    "\n",
    "This notebook is the recommended walkthrough for the `pprl-superhero-example` demo. It now follows the **ECDH public-key exchange** flow end-to-end (no pre-shared secrets). It shows how two organizations can link records **without exchanging raw identifiers** (like names or Social Security Numbers).\n",
    "\n",
    "**Scenario:** Super Hero Hospital and Super Hero Pharmacy want to link patient records for care coordination while protecting privacy.\n",
    "\n",
    "**Who does what:** In this demo, assume the **hospital** runs the overlap analysis step (it receives the pharmacy token package and compares tokens to find matches).\n",
    "\n",
    "**Match policy:** OpenToken provides standard token rules (T1–T5), but it does **not** define a single, universal match policy. A match policy is what the parties agree on: **which token IDs must match (and how many)** before treating two records as the same person. This notebook shows strict matching (T1–T5).\n",
    "\n",
    "**Prefer a one-command run?** From this directory you can run `./run_end_to_end.sh` to generate data, perform ECDH key exchange, tokenize, decrypt, and analyze overlap end-to-end (see `README.md`).\n",
    "\n",
    "> **Important demo disclaimer:** This example uses fully synthetic data and demo keys that are safe for illustration only. Do not reuse these keys or patterns in production. Real deployments must use strong key management, strict access controls around decryption and matching, and clear governance over who can run linkage jobs and how results are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902052fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports + paths\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _find_demo_dir() -> Path:\n",
    "    \"\"\"Find the demo directory regardless of the current working directory.\"\"\"\n",
    "    start = Path.cwd().resolve()\n",
    "    candidates = [start, *start.parents]\n",
    "    for base in candidates:\n",
    "        # Case 1: notebook opened from the demo directory\n",
    "        if (base / \"scripts\" / \"generate_superhero_datasets.py\").exists():\n",
    "            return base\n",
    "        # Case 2: notebook opened from repo root (or elsewhere)\n",
    "        demo = base / \"demos\" / \"pprl-superhero-example\"\n",
    "        if (demo / \"scripts\" / \"generate_superhero_datasets.py\").exists():\n",
    "            return demo\n",
    "    raise FileNotFoundError(\"Could not locate demos/pprl-superhero-example\")\n",
    "\n",
    "\n",
    "demo_dir = _find_demo_dir()\n",
    "scripts_dir = demo_dir / \"scripts\"\n",
    "datasets_dir = demo_dir / \"datasets\"\n",
    "outputs_dir = demo_dir / \"outputs\"\n",
    "keys_dir = demo_dir / \"keys\"\n",
    "\n",
    "# Ensure expected folders exist\n",
    "datasets_dir.mkdir(parents=True, exist_ok=True)\n",
    "outputs_dir.mkdir(parents=True, exist_ok=True)\n",
    "keys_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Demo directory: {demo_dir}\")\n",
    "print(\"Using ECDH public-key exchange demo flow (no shared secrets).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18cddc",
   "metadata": {},
   "source": [
    "## 2. Generate Superhero Datasets\n",
    "\n",
    "Create two datasets (hospital and pharmacy) with a 40% overlap. The overlap represents patients that appear in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66767d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the data generation script\n",
    "result = subprocess.run(\n",
    "    [sys.executable, str(scripts_dir / \"generate_superhero_datasets.py\")],\n",
    "    cwd=str(demo_dir),\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    check=False,\n",
    " )\n",
    "\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print(f\"Error: {result.stderr}\")\n",
    "    raise RuntimeError(\"Dataset generation failed\")\n",
    "print(\"✓ Datasets generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3980ca",
   "metadata": {},
   "source": [
    "### Inspect the Generated Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643634a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display hospital dataset\n",
    "hospital_df = pd.read_csv(datasets_dir / 'hospital_superhero_data.csv')\n",
    "print(f\"Hospital Dataset: {len(hospital_df)} records\")\n",
    "print(hospital_df.head())\n",
    "print()\n",
    "\n",
    "# Load and display pharmacy dataset\n",
    "pharmacy_df = pd.read_csv(datasets_dir / 'pharmacy_superhero_data.csv')\n",
    "print(f\"Pharmacy Dataset: {len(pharmacy_df)} records\")\n",
    "print(pharmacy_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ebaa01",
   "metadata": {},
   "source": [
    "## 3. Tokenize the Datasets (ECDH via demo scripts)\n",
    "\n",
    "Each organization tokenizes their data independently using **ECDH public-key exchange**. In this notebook we call the demo scripts (Java/Python under the hood) to perform:\n",
    "1. Generate pharmacy key pair (receiver)\n",
    "2. Hospital tokenization with ECDH (sender)\n",
    "3. Pharmacy decryption + overlap analysis inputs\n",
    "\n",
    "**Note:** The end-to-end script (`run_end_to_end.sh`) runs the same flow in one command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokenizing datasets with ECDH demo scripts...\")\n",
    "print()\n",
    "\n",
    "# Scripts (ECDH flow only)\n",
    "pharmacy_keys_script = scripts_dir / \"tokenize_pharmacy_generate_keys.sh\"\n",
    "hospital_script = scripts_dir / \"tokenize_hospital.sh\"\n",
    "pharmacy_decrypt_script = scripts_dir / \"tokenize_pharmacy_decrypt.sh\"\n",
    "\n",
    "# Ensure executability\n",
    "for script in [pharmacy_keys_script, hospital_script, pharmacy_decrypt_script]:\n",
    "    subprocess.run([\"chmod\", \"+x\", str(script)], cwd=str(demo_dir), check=False)\n",
    "\n",
    "# Step A: Pharmacy generates key pair\n",
    "result = subprocess.run(\n",
    "    [str(pharmacy_keys_script)],\n",
    "    cwd=str(demo_dir),\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    check=False,\n",
    ")\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print(f\"Error: {result.stderr}\")\n",
    "    raise RuntimeError(\"Key generation failed\")\n",
    "print(\"✓ Pharmacy ECDH key pair generated\")\n",
    "print()\n",
    "\n",
    "# Step B: Hospital tokenizes with ECDH (outputs hospital_tokens_ecdh.zip)\n",
    "result = subprocess.run(\n",
    "    [str(hospital_script)],\n",
    "    cwd=str(demo_dir),\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    check=False,\n",
    ")\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print(f\"Error: {result.stderr}\")\n",
    "    raise RuntimeError(\"Hospital tokenization failed\")\n",
    "print(\"✓ Hospital tokenization (ECDH) completed\")\n",
    "print()\n",
    "\n",
    "# Step C: Pharmacy decrypts hospital tokens and prepares for analysis\n",
    "result = subprocess.run(\n",
    "    [str(pharmacy_decrypt_script)],\n",
    "    cwd=str(demo_dir),\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    check=False,\n",
    ")\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print(f\"Error: {result.stderr}\")\n",
    "    raise RuntimeError(\"Pharmacy decryption failed\")\n",
    "print(\"✓ Pharmacy decryption completed; decrypted tokens ready for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefeb79d",
   "metadata": {},
   "source": [
    "### Inspect Tokenized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f011cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decrypted hospital tokens (produced by ECDH flow)\n",
    "pharmacy_decrypted = pd.read_csv(outputs_dir / \"pharmacy_decrypted_hospital_tokens.csv\")\n",
    "print(f\"Decrypted Hospital Tokens (from pharmacy): {len(pharmacy_decrypted)} token rows\")\n",
    "print(pharmacy_decrypted.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4935eeaf",
   "metadata": {},
   "source": [
    "## 4. Decrypt Tokens and Perform Overlap Analysis (ECDH)\n",
    "\n",
    "To compare tokens across independently tokenized datasets:\n",
    "1. The hospital sends encrypted tokens + its public key (in `hospital_tokens_ecdh.zip`).\n",
    "2. The pharmacy uses its private key + the hospital public key to derive the same keys (ECDH) and decrypts to the deterministic HMAC layer.\n",
    "3. The pharmacy compares the decrypted fingerprints to find matching records.\n",
    "\n",
    "**Why decryption is needed:** OpenToken uses random IVs for encryption, so even identical patients produce different encrypted token strings. Decryption reveals the deterministic fingerprint layer that can be compared for equality.\n",
    "\n",
    "**Who runs this step (in this demo):** the **pharmacy** runs the overlap analysis after decrypting (trusted environment with its private key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c8ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performing overlap analysis (ECDH)...\")\n",
    "print()\n",
    "\n",
    "analyze_script = scripts_dir / \"analyze_overlap.py\"\n",
    "result = subprocess.run(\n",
    "    [sys.executable, str(analyze_script)],\n",
    "    cwd=str(demo_dir),\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    check=False,\n",
    ")\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print(f\"Error: {result.stderr}\")\n",
    "    raise RuntimeError(\"Overlap analysis failed\")\n",
    "print(\"✓ Overlap analysis completed successfully (ECDH)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d2c41",
   "metadata": {},
   "source": [
    "### View Matching Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display matching results (ECDH)\n",
    "matches_df = pd.read_csv(outputs_dir / \"matching_records_ecdh.csv\")\n",
    "\n",
    "print(f\"Total Matching Pairs: {len(matches_df)}\")\n",
    "print()\n",
    "print(\"First 10 matching records:\")\n",
    "print(matches_df.head(10))\n",
    "print()\n",
    "\n",
    "# Summary statistics\n",
    "hospital_count = len(pd.read_csv(datasets_dir / \"hospital_superhero_data.csv\"))\n",
    "pharmacy_count = len(pd.read_csv(datasets_dir / \"pharmacy_superhero_data.csv\"))\n",
    "unique_hospital_matches = matches_df['Hospital_RecordId'].nunique()\n",
    "unique_pharmacy_matches = matches_df['Pharmacy_RecordId'].nunique()\n",
    "\n",
    "print(\"Summary Statistics:\")\n",
    "print(f\"- Hospital records with matches: {unique_hospital_matches} out of {hospital_count}\")\n",
    "print(f\"- Pharmacy records with matches: {unique_pharmacy_matches} out of {pharmacy_count}\")\n",
    "print(f\"- Overlap percentage (hospital): {(unique_hospital_matches / hospital_count * 100):.1f}%\")\n",
    "print(f\"- Overlap percentage (pharmacy): {(unique_pharmacy_matches / pharmacy_count * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa68179",
   "metadata": {},
   "source": [
    "## 5. Alternative Analysis\n",
    "\n",
    "For custom match rules (e.g., fewer than 5 tokens), rerun the CLI overlap analyzer with your desired `matching_rules` configuration in `scripts/analyze_overlap.py`. This notebook run uses the default strict T1–T5 policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Alternative analysis not run here.\")\n",
    "print(\"To try different matching rules, edit scripts/analyze_overlap.py and rerun the notebook or CLI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f8c62",
   "metadata": {},
   "source": [
    "### Interpreting the Alternative Analysis\n",
    "\n",
    "The alternative analysis uses only 4 tokens (T1, T2, T3, T5) instead of all 5:\n",
    "- **T1**: FirstName + LastName + Sex + BirthDate\n",
    "- **T2**: FirstName + LastName + PostalCode\n",
    "- **T3**: FirstName + LastName + SocialSecurityNumber\n",
    "- **T4**: ❌ *EXCLUDED* - BirthDate + Sex + PostalCode\n",
    "- **T5**: BirthDate + Sex + SocialSecurityNumber\n",
    "\n",
    "By excluding T4, we're being more lenient about postal code consistency. This might find additional matches where:\n",
    "- Postal codes have typos or formatting differences\n",
    "- People have moved between visits\n",
    "- Data entry errors occurred\n",
    "\n",
    "However, this also increases the risk of false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8deb8",
   "metadata": {},
   "source": [
    "## 6. Understand the Results\n",
    "\n",
    "Let's look at what a match actually means by examining some matched records in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24fa176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample matched record\n",
    "if len(matches_df) > 0:\n",
    "    sample_match = matches_df.iloc[0]\n",
    "    hospital_record_id = sample_match['Hospital_RecordId']\n",
    "    pharmacy_record_id = sample_match['Pharmacy_RecordId']\n",
    "    \n",
    "    # Get the original records\n",
    "    hospital_match = pd.read_csv(datasets_dir / \"hospital_superhero_data.csv\")\n",
    "    hospital_match = hospital_match[hospital_match['RecordId'] == hospital_record_id]\n",
    "    pharmacy_match = pd.read_csv(datasets_dir / \"pharmacy_superhero_data.csv\")\n",
    "    pharmacy_match = pharmacy_match[pharmacy_match['RecordId'] == pharmacy_record_id]\n",
    "    \n",
    "    if len(hospital_match) == 0 or len(pharmacy_match) == 0:\n",
    "        print(f\"Warning: Could not find matching records in original datasets\")\n",
    "        print(f\"Hospital RecordId {hospital_record_id} found: {len(hospital_match) > 0}\")\n",
    "        print(f\"Pharmacy RecordId {pharmacy_record_id} found: {len(pharmacy_match) > 0}\")\n",
    "    else:\n",
    "        hospital_record = hospital_match.iloc[0]\n",
    "        pharmacy_record = pharmacy_match.iloc[0]\n",
    "        \n",
    "        print(\"Sample Match:\")\n",
    "        print(f\"Hospital Record ID: {hospital_record_id}\")\n",
    "        print(f\"Hospital Patient: {hospital_record['FirstName']} {hospital_record['LastName']}\")\n",
    "        print(f\"DOB: {hospital_record['BirthDate']}, SSN: {hospital_record['SocialSecurityNumber']}\")\n",
    "        print()\n",
    "        print(f\"Pharmacy Record ID: {pharmacy_record_id}\")\n",
    "        print(f\"Pharmacy Patient: {pharmacy_record['FirstName']} {pharmacy_record['LastName']}\")\n",
    "        print(f\"DOB: {pharmacy_record['BirthDate']}, SSN: {pharmacy_record['SocialSecurityNumber']}\")\n",
    "        print()\n",
    "        print(\"✓ All 5 tokens matched, confirming this is the same patient!\")\n",
    "else:\n",
    "    print(\"No matches found. This could happen if:\")\n",
    "    print(\"- Key exchange was not run (tokenization/decryption failed)\")\n",
    "    print(\"- Data validation rejected records with invalid attributes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef457335",
   "metadata": {},
   "source": [
    "## 7. Advanced PySpark Transformations (Optional)\n",
    "\n",
    "If PySpark is available, we can perform distributed transformations on the tokenized data for large-scale analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a16870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PySpark availability\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    pyspark_available = True\n",
    "    \n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"OpenToken PPRL Demo\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"✓ PySpark is available\")\n",
    "    print(f\"Spark version: {spark.version}\")\n",
    "except ImportError:\n",
    "    pyspark_available = False\n",
    "    spark = None\n",
    "    print(\"PySpark not installed - advanced transformations will be skipped\")\n",
    "    print(\"To enable PySpark: pip install pyspark opentoken-pyspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark-based transformations for distributed processing\n",
    "if not (pyspark_available and spark):\n",
    "    print(\"PySpark not available for advanced transformations.\")\n",
    "    print(\"Core PPRL analysis completed successfully using pandas.\")\n",
    "elif not (outputs_dir / 'pharmacy_decrypted_hospital_tokens.csv').exists():\n",
    "    print(\"PySpark analysis skipped for ECDH flow.\")\n",
    "    print(\"ECDH produces encrypted tokens in ZIP format and decrypted tokens for analysis.\")\n",
    "    print(\"For PySpark analysis, extract tokens from ZIP or tokenize with pre-shared secrets.\")\n",
    "else:\n",
    "    try:\n",
    "        from pyspark.sql.functions import col, count as spark_count\n",
    "        \n",
    "        print(\"Performing distributed token analysis with PySpark (ECDH flow)...\")\n",
    "        print()\n",
    "        \n",
    "        # Load the ECDH-decrypted hospital tokens\n",
    "        hospital_tokens_spark = spark.read.csv(\n",
    "            str(outputs_dir / 'pharmacy_decrypted_hospital_tokens.csv'),\n",
    "            header=True,\n",
    "            inferSchema=False  # Keep all as strings\n",
    "        )\n",
    "        \n",
    "        # Analyze token distribution in decrypted hospital dataset\n",
    "        print(\"Hospital Token Distribution (ECDH-decrypted):\")\n",
    "        hospital_tokens_spark.groupBy(\"RuleId\").agg(spark_count(\"*\").alias(\"count\")).orderBy(\"RuleId\").show()\n",
    "        print()\n",
    "        \n",
    "        # Count unique records\n",
    "        hospital_unique = hospital_tokens_spark.select(\"RecordId\").distinct().count()\n",
    "        print(f\"Unique hospital records: {hospital_unique}\")\n",
    "        print()\n",
    "        print(\"Note: ECDH flow analyzed. For pharmacy token distribution, run pharmacy\")\n",
    "        print(\"      tokenization separately (not covered in this ECDH demo).\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Note: Advanced transformations not available - {type(e).__name__}\")\n",
    "        print(\"This is optional and does not affect the core PPRL workflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35422623",
   "metadata": {},
   "source": [
    "## 8. Privacy and Security Summary\n",
    "\n",
    "This demonstration shows how OpenToken enables privacy-preserving record linkage:\n",
    "\n",
    "### What was protected:\n",
    "- ✓ Raw patient data (names, SSNs, birthdates) was never shared between organizations\n",
    "- ✓ HMAC-SHA256 hashes cannot be reversed to recover original data\n",
    "- ✓ Encryption key controls who can decrypt and perform linkage\n",
    "\n",
    "### What was shared:\n",
    "- • Encrypted tokens for secure transmission\n",
    "- • Matching statistics showing overlap counts\n",
    "- • Metadata with summary information (not raw data)\n",
    "\n",
    "### Key security principles:\n",
    "1. **Strong Encryption**: AES-256-GCM with random IVs prevents pattern analysis\n",
    "2. **Key Management**: Secure sharing and storage of encryption/hashing keys\n",
    "3. **Deterministic Hashing**: HMAC-SHA256 enables comparison without raw data\n",
    "4. **Access Control**: Only authorized parties can decrypt tokens\n",
    "\n",
    "### PySpark Bridge Benefits:\n",
    "- **Distributed Processing**: Handle large datasets across multiple nodes\n",
    "- **Parallel Decryption**: Efficiently decrypt millions of tokens\n",
    "- **Scalable Analysis**: Perform overlap analysis on enterprise-scale data\n",
    "- **Integration**: Native Spark SQL for additional transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f42e1d",
   "metadata": {},
   "source": [
    "## 9. Customization Examples\n",
    "\n",
    "You can customize this demo by modifying the scripts:\n",
    "\n",
    "### Change dataset size and overlap:\n",
    "Edit `scripts/generate_superhero_datasets.py`:\n",
    "```python\n",
    "num_hospital = 500  # Different size\n",
    "num_pharmacy = 600\n",
    "overlap_percentage = 0.50  # 50% overlap instead of 40%\n",
    "```\n",
    "\n",
    "### Use different secrets (demo only):\n",
    "Edit **both** tokenization scripts to keep them in sync:\n",
    "- `scripts/tokenize_hospital.sh`\n",
    "- `scripts/tokenize_pharmacy.sh`\n",
    "\n",
    "```bash\n",
    "HASHING_SECRET=\"YourCustomHashingKey\"\n",
    "ENCRYPTION_KEY=\"YourCustomEncryptionKey-32chars\"  # Must be exactly 32 characters\n",
    "```\n",
    "\n",
    "**Important**: Both organizations must use the same secrets for tokens to match.\n",
    "\n",
    "### Scale with PySpark:\n",
    "For large datasets, ensure PySpark and the OpenToken PySpark bridge are installed:\n",
    "```bash\n",
    "pip install pyspark opentoken-pyspark\n",
    "```\n",
    "\n",
    "The notebook will automatically use distributed processing if available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a16e4",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "This PPRL demo can be adapted for:\n",
    "- Healthcare: Hospital-to-hospital patient matching\n",
    "- Insurance: Claims linkage across providers\n",
    "- Research: Multi-site study participant matching\n",
    "- Government: Cross-agency identity resolution\n",
    "- Financial Services: Anti-fraud systems\n",
    "\n",
    "### With PySpark Bridge:\n",
    "- Scale to petabyte-level datasets\n",
    "- Distribute tokenization across clusters\n",
    "- Parallel overlap analysis\n",
    "- Real-time record linkage pipelines\n",
    "\n",
    "For more information, see the [README.md](./README.md) in this directory and the [main OpenToken documentation](../../README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca8862c",
   "metadata": {},
   "source": [
    "<!-- notebook-edit-test -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
