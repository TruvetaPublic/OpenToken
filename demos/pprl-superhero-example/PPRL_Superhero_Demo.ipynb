{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d416c681",
   "metadata": {},
   "source": [
    "# Privacy-Preserving Record Linkage (PPRL) - Superhero Demo with PySpark\n",
    "\n",
    "This notebook demonstrates how OpenToken enables privacy-preserving record linkage between two organizations without sharing sensitive patient data.\n",
    "\n",
    "**Scenario**: Super Hero Hospital and Super Hero Pharmacy want to link patient records for care coordination while protecting patient privacy.\n",
    "\n",
    "**PySpark Bridge**: This notebook uses the OpenToken PySpark bridge for distributed overlap analysis and advanced transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83162fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up paths\n",
    "demo_dir = Path.cwd()\n",
    "scripts_dir = demo_dir / 'scripts'\n",
    "datasets_dir = demo_dir / 'datasets'\n",
    "outputs_dir = demo_dir / 'outputs'\n",
    "\n",
    "print(f\"Demo directory: {demo_dir}\")\n",
    "print(f\"Scripts directory: {scripts_dir}\")\n",
    "print(f\"Datasets directory: {datasets_dir}\")\n",
    "print(f\"Outputs directory: {outputs_dir}\")\n",
    "\n",
    "# Try to import PySpark components (optional)\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from opentoken_pyspark.overlap_analyzer import OpenTokenOverlapAnalyzer\n",
    "    pyspark_available = True\n",
    "    print(\"✓ PySpark and OpenToken PySpark Bridge available\")\n",
    "except ImportError:\n",
    "    pyspark_available = False\n",
    "    print(\"⚠ PySpark not available - will use pandas-based analysis instead\")\n",
    "    print(\"  (Install with: pip install pyspark opentoken-pyspark)\")\n",
    "\n",
    "# Initialize Spark Session if available\n",
    "spark = None\n",
    "if pyspark_available:\n",
    "    try:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"PPRL-Superhero-Demo\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "            .getOrCreate()\n",
    "        print(\"✓ Spark Session initialized\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Could not initialize Spark: {e}\")\n",
    "        pyspark_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18cddc",
   "metadata": {},
   "source": [
    "## 2. Generate Superhero Datasets\n",
    "\n",
    "Create two datasets (hospital and pharmacy) with a 40% overlap. The overlap represents patients that appear in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66767d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the data generation script\n",
    "result = subprocess.run(\n",
    "    ['python', str(scripts_dir / 'generate_superhero_datasets.py')],\n",
    "    cwd=str(demo_dir),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print(f\"Error: {result.stderr}\")\n",
    "else:\n",
    "    print(\"✓ Datasets generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3980ca",
   "metadata": {},
   "source": [
    "### Inspect the Generated Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643634a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display hospital dataset\n",
    "hospital_df = pd.read_csv(datasets_dir / 'hospital_superhero_data.csv')\n",
    "print(f\"Hospital Dataset: {len(hospital_df)} records\")\n",
    "print(hospital_df.head())\n",
    "print()\n",
    "\n",
    "# Load and display pharmacy dataset\n",
    "pharmacy_df = pd.read_csv(datasets_dir / 'pharmacy_superhero_data.csv')\n",
    "print(f\"Pharmacy Dataset: {len(pharmacy_df)} records\")\n",
    "print(pharmacy_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ebaa01",
   "metadata": {},
   "source": [
    "## 3. Tokenize the Datasets\n",
    "\n",
    "Each organization tokenizes their data independently using the OpenToken Java CLI. This applies:\n",
    "1. HMAC-SHA256 hashing for deterministic tokens\n",
    "2. AES-256-GCM encryption for secure transmission\n",
    "\n",
    "**Important**: Both organizations use the same hashing and encryption keys to enable later comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the tokenization script executable\n",
    "tokenize_script = scripts_dir / 'tokenize_datasets.sh'\n",
    "os.chmod(tokenize_script, 0o755)\n",
    "\n",
    "# Run tokenization\n",
    "result = subprocess.run(\n",
    "    ['bash', str(tokenize_script)],\n",
    "    cwd=str(demo_dir),\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    env={**os.environ, 'PATH': os.environ.get('PATH', '')}\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print(f\"Error: {result.stderr}\")\n",
    "    raise RuntimeError(\"Tokenization failed\")\n",
    "else:\n",
    "    print(\"✓ Tokenization completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefeb79d",
   "metadata": {},
   "source": [
    "### Inspect Tokenized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f011cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized hospital data\n",
    "hospital_tokens = pd.read_csv(outputs_dir / 'hospital_tokens.csv')\n",
    "print(f\"Hospital Tokens: {len(hospital_tokens)} token rows (5 per patient)\")\n",
    "print(hospital_tokens.head(10))\n",
    "print()\n",
    "\n",
    "# Load tokenized pharmacy data\n",
    "pharmacy_tokens = pd.read_csv(outputs_dir / 'pharmacy_tokens.csv')\n",
    "print(f\"Pharmacy Tokens: {len(pharmacy_tokens)} token rows (5 per patient)\")\n",
    "print(pharmacy_tokens.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81835bde",
   "metadata": {},
   "source": [
    "### View Metadata\n",
    "\n",
    "The metadata files contain information about the tokenization process, including record counts and hashes of the secrets used (not the secrets themselves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f5f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display hospital metadata\n",
    "with open(outputs_dir / 'hospital_tokens.csv.metadata.json') as f:\n",
    "    hospital_metadata = json.load(f)\n",
    "    \n",
    "print(\"Hospital Metadata:\")\n",
    "print(json.dumps(hospital_metadata, indent=2))\n",
    "print()\n",
    "\n",
    "# Load and display pharmacy metadata\n",
    "with open(outputs_dir / 'pharmacy_tokens.csv.metadata.json') as f:\n",
    "    pharmacy_metadata = json.load(f)\n",
    "    \n",
    "print(\"Pharmacy Metadata:\")\n",
    "print(json.dumps(pharmacy_metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4935eeaf",
   "metadata": {},
   "source": [
    "## 4. Decrypt Tokens and Perform Overlap Analysis\n",
    "\n",
    "To compare tokens across independently tokenized datasets:\n",
    "1. **Decrypt** the encrypted tokens to reveal the underlying HMAC-SHA256 hashes\n",
    "2. **Compare** the decrypted hashes to find matching records\n",
    "\n",
    "**Why decryption is needed**: OpenToken uses random IVs for encryption, so even identical patients produce different encrypted tokens. Decryption reveals the deterministic hash layer that can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c8ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If PySpark is available, use the PySpark bridge for analysis\n",
    "if pyspark_available and spark:\n",
    "    print(\"Using OpenToken PySpark Bridge for overlap analysis...\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        # Load tokenized data into Spark DataFrames\n",
    "        hospital_tokens_spark = spark.read.csv(\n",
    "            str(outputs_dir / 'hospital_tokens.csv'),\n",
    "            header=True,\n",
    "            inferSchema=True\n",
    "        )\n",
    "        pharmacy_tokens_spark = spark.read.csv(\n",
    "            str(outputs_dir / 'pharmacy_tokens.csv'),\n",
    "            header=True,\n",
    "            inferSchema=True\n",
    "        )\n",
    "        \n",
    "        # Initialize the overlap analyzer with the encryption key\n",
    "        encryption_key = \"Secret-Encryption-Key-Goes-Here.\"  # Same key used for tokenization\n",
    "        analyzer = OpenTokenOverlapAnalyzer(encryption_key)\n",
    "        \n",
    "        # Analyze overlap using PySpark\n",
    "        matches_spark = analyzer.analyze_overlap(\n",
    "            hospital_tokens_spark,\n",
    "            pharmacy_tokens_spark,\n",
    "            [\"T1\", \"T2\", \"T3\", \"T4\", \"T5\"],  # All 5 tokens must match\n",
    "            hospital_record_id_column=\"RecordId\",\n",
    "            pharmacy_record_id_column=\"RecordId\",\n",
    "            hospital_token_column=\"Token\",\n",
    "            pharmacy_token_column=\"Token\",\n",
    "            hospital_token_id_column=\"TokenId\",\n",
    "            pharmacy_token_id_column=\"TokenId\"\n",
    "        )\n",
    "        \n",
    "        # Collect results for display\n",
    "        matches_list = matches_spark.collect()\n",
    "        print(f\"✓ Overlap analysis completed using PySpark!\")\n",
    "        print(f\"  Found {len(matches_list)} matching record pairs\")\n",
    "        \n",
    "        # Show results\n",
    "        matches_spark.show(10)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during PySpark analysis: {e}\")\n",
    "        print(\"Falling back to pandas-based analysis...\")\n",
    "        pyspark_available = False\n",
    "\n",
    "# Fallback to pandas-based analysis\n",
    "if not pyspark_available or not spark:\n",
    "    print(\"Using pandas-based overlap analysis...\")\n",
    "    print()\n",
    "    \n",
    "    # Make the overlap analysis script executable\n",
    "    analyze_script = scripts_dir / 'analyze_overlap.py'\n",
    "\n",
    "    # Run overlap analysis\n",
    "    result = subprocess.run(\n",
    "        ['python', str(analyze_script)],\n",
    "        cwd=str(demo_dir),\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "    else:\n",
    "        print(\"✓ Overlap analysis completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d2c41",
   "metadata": {},
   "source": [
    "### View Matching Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display matching results\n",
    "matches_df = pd.read_csv(outputs_dir / 'matching_records.csv')\n",
    "\n",
    "print(f\"Total Matching Pairs: {len(matches_df)}\")\n",
    "print()\n",
    "print(\"First 10 matching records:\")\n",
    "print(matches_df.head(10))\n",
    "print()\n",
    "\n",
    "# Summary statistics\n",
    "hospital_count = len(hospital_df)\n",
    "pharmacy_count = len(pharmacy_df)\n",
    "unique_hospital_matches = matches_df['HospitalRecordId'].nunique()\n",
    "unique_pharmacy_matches = matches_df['PharmacyRecordId'].nunique()\n",
    "\n",
    "print(\"Summary Statistics:\")\n",
    "print(f\"- Hospital records with matches: {unique_hospital_matches} out of {hospital_count}\")\n",
    "print(f\"- Pharmacy records with matches: {unique_pharmacy_matches} out of {pharmacy_count}\")\n",
    "print(f\"- Overlap percentage (hospital): {(unique_hospital_matches / hospital_count * 100):.1f}%\")\n",
    "print(f\"- Overlap percentage (pharmacy): {(unique_pharmacy_matches / pharmacy_count * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd827143",
   "metadata": {},
   "source": [
    "## 6. Understand the Results\n",
    "\n",
    "Let's look at what a match actually means by examining some matched records in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24fa176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample matched record\n",
    "if len(matches_df) > 0:\n",
    "    sample_match = matches_df.iloc[0]\n",
    "    hospital_record_id = sample_match['HospitalRecordId']\n",
    "    pharmacy_record_id = sample_match['PharmacyRecordId']\n",
    "    \n",
    "    # Get the original records\n",
    "    hospital_record = hospital_df[hospital_df['RecordId'] == hospital_record_id].iloc[0]\n",
    "    pharmacy_record = pharmacy_df[pharmacy_df['RecordId'] == pharmacy_record_id].iloc[0]\n",
    "    \n",
    "    print(\"Sample Match:\")\n",
    "    print(f\"Hospital Record ID: {hospital_record_id}\")\n",
    "    print(f\"Hospital Patient: {hospital_record['FirstName']} {hospital_record['LastName']}\")\n",
    "    print(f\"DOB: {hospital_record['BirthDate']}, SSN: {hospital_record['SocialSecurityNumber']}\")\n",
    "    print()\n",
    "    print(f\"Pharmacy Record ID: {pharmacy_record_id}\")\n",
    "    print(f\"Pharmacy Patient: {pharmacy_record['FirstName']} {pharmacy_record['LastName']}\")\n",
    "    print(f\"DOB: {pharmacy_record['BirthDate']}, SSN: {pharmacy_record['SocialSecurityNumber']}\")\n",
    "    print()\n",
    "    print(\"✓ All 5 tokens matched, confirming this is the same patient!\")\n",
    "else:\n",
    "    print(\"No matches found. This could happen if:\")\n",
    "    print(\"- Different hashing/encryption keys were used\")\n",
    "    print(\"- Data validation rejected records with invalid attributes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef457335",
   "metadata": {},
   "source": [
    "## 5a. Advanced PySpark Transformations (Optional)\n",
    "\n",
    "If PySpark is available, we can perform distributed transformations on the tokenized data for large-scale analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark-based transformations for distributed processing\n",
    "if pyspark_available and spark:\n",
    "    try:\n",
    "        from pyspark.sql.functions import explode, col, count as spark_count\n",
    "        \n",
    "        print(\"Performing distributed token analysis with PySpark...\")\n",
    "        print()\n",
    "        \n",
    "        # Load the tokenized data\n",
    "        hospital_tokens_spark = spark.read.csv(\n",
    "            str(outputs_dir / 'hospital_tokens.csv'),\n",
    "            header=True,\n",
    "            inferSchema=True\n",
    "        )\n",
    "        pharmacy_tokens_spark = spark.read.csv(\n",
    "            str(outputs_dir / 'pharmacy_tokens.csv'),\n",
    "            header=True,\n",
    "            inferSchema=True\n",
    "        )\n",
    "        \n",
    "        # Analyze token distribution in hospital dataset\n",
    "        print(\"Hospital Token Distribution:\")\n",
    "        hospital_tokens_spark.groupBy(\"TokenId\").agg(spark_count(\"*\").alias(\"count\")).show()\n",
    "        print()\n",
    "        \n",
    "        # Analyze token distribution in pharmacy dataset\n",
    "        print(\"Pharmacy Token Distribution:\")\n",
    "        pharmacy_tokens_spark.groupBy(\"TokenId\").agg(spark_count(\"*\").alias(\"count\")).show()\n",
    "        print()\n",
    "        \n",
    "        # Count unique records\n",
    "        hospital_unique = hospital_tokens_spark.select(\"RecordId\").distinct().count()\n",
    "        pharmacy_unique = pharmacy_tokens_spark.select(\"RecordId\").distinct().count()\n",
    "        print(f\"Unique records - Hospital: {hospital_unique}, Pharmacy: {pharmacy_unique}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Note: Advanced transformations not available - {type(e).__name__}\")\n",
    "        print(\"This is optional and does not affect the core PPRL workflow.\")\n",
    "else:\n",
    "    print(\"PySpark not available for advanced transformations.\")\n",
    "    print(\"Core PPRL analysis completed successfully using pandas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35422623",
   "metadata": {},
   "source": [
    "## 7. Privacy and Security Summary\n",
    "\n",
    "This demonstration shows how OpenToken enables privacy-preserving record linkage:\n",
    "\n",
    "### What was protected:\n",
    "- ✓ Raw patient data (names, SSNs, birthdates) was never shared between organizations\n",
    "- ✓ HMAC-SHA256 hashes cannot be reversed to recover original data\n",
    "- ✓ Encryption key controls who can decrypt and perform linkage\n",
    "\n",
    "### What was shared:\n",
    "- • Encrypted tokens for secure transmission\n",
    "- • Matching statistics showing overlap counts\n",
    "- • Metadata with summary information (not raw data)\n",
    "\n",
    "### Key security principles:\n",
    "1. **Strong Encryption**: AES-256-GCM with random IVs prevents pattern analysis\n",
    "2. **Key Management**: Secure sharing and storage of encryption/hashing keys\n",
    "3. **Deterministic Hashing**: HMAC-SHA256 enables comparison without raw data\n",
    "4. **Access Control**: Only authorized parties can decrypt tokens\n",
    "\n",
    "### PySpark Bridge Benefits:\n",
    "- **Distributed Processing**: Handle large datasets across multiple nodes\n",
    "- **Parallel Decryption**: Efficiently decrypt millions of tokens\n",
    "- **Scalable Analysis**: Perform overlap analysis on enterprise-scale data\n",
    "- **Integration**: Native Spark SQL for additional transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f42e1d",
   "metadata": {},
   "source": [
    "## 8. Customization Examples\n",
    "\n",
    "You can customize this demo by modifying the scripts:\n",
    "\n",
    "### Change dataset size and overlap:\n",
    "Edit `scripts/generate_superhero_datasets.py`:\n",
    "```python\n",
    "num_hospital = 500  # Different size\n",
    "num_pharmacy = 600\n",
    "overlap_percentage = 0.50  # 50% overlap instead of 40%\n",
    "```\n",
    "\n",
    "### Use different encryption keys:\n",
    "Edit `scripts/tokenize_datasets.sh`:\n",
    "```bash\n",
    "HASH_KEY=\"YourCustomHashingKey\"\n",
    "ENCRYPTION_KEY=\"YourCustomEncryptionKey-32\"\n",
    "```\n",
    "\n",
    "**Important**: Both organizations must use the same keys for tokens to match!\n",
    "\n",
    "### Scale with PySpark:\n",
    "For large datasets, ensure PySpark is installed:\n",
    "```bash\n",
    "pip install pyspark opentoken-pyspark\n",
    "```\n",
    "\n",
    "The notebook will automatically use distributed processing if available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a16e4",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "This PPRL demo can be adapted for:\n",
    "- Healthcare: Hospital-to-hospital patient matching\n",
    "- Insurance: Claims linkage across providers\n",
    "- Research: Multi-site study participant matching\n",
    "- Government: Cross-agency identity resolution\n",
    "- Financial Services: Anti-fraud systems\n",
    "\n",
    "### With PySpark Bridge:\n",
    "- Scale to petabyte-level datasets\n",
    "- Distribute tokenization across clusters\n",
    "- Parallel overlap analysis\n",
    "- Real-time record linkage pipelines\n",
    "\n",
    "For more information, see the [README.md](./README.md) in this directory and the [main OpenToken documentation](../../README.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
