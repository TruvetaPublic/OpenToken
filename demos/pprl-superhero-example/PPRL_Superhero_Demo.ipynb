{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d416c681",
   "metadata": {},
   "source": [
    "# Privacy-Preserving Record Linkage (PPRL) Demonstration\n",
    "\n",
    "This notebook is the recommended walkthrough for the `pprl-superhero-example` demo. It shows how two organizations can link records **without exchanging raw identifiers** (like names or Social Security Numbers).\n",
    "\n",
    "**Scenario:** Super Hero Hospital and Super Hero Pharmacy want to link patient records for care coordination while protecting privacy.\n",
    "\n",
    "**Who does what:** In this demo, assume the **hospital** runs the overlap analysis step (it receives the pharmacy token file and compares tokens to find matches).\n",
    "\n",
    "**Match policy:** OpenToken provides standard token rules (T1–T5), but it does **not** define a single, universal match policy. A match policy is what the parties agree on: **which token IDs must match (and how many)** before treating two records as the same person. This notebook shows strict matching (T1–T5) and an example of relaxing the policy.\n",
    "\n",
    "**Custom tokens (optional):** Parties can define additional token rules beyond T1–T5, as long as both sides use the exact same token definitions and normalization rules (otherwise tokens won’t be comparable).\n",
    "\n",
    "**Prefer a one-command run?** From this directory you can run `./run_end_to_end.sh` to generate data, tokenize, and analyze overlap end-to-end (see `README.md`).\n",
    "\n",
    "> **Important demo disclaimer:** This example uses fully synthetic data and example secrets that are safe for illustration only. Do not reuse these keys or patterns in production. Real deployments must use strong key management, strict access controls around decryption and matching, and clear governance over who can run linkage jobs and how results are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902052fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: imports + paths + optional PySpark bridge\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _find_demo_dir() -> Path:\n",
    "    \"\"\"Find the demo directory regardless of the current working directory.\"\"\"\n",
    "    start = Path.cwd().resolve()\n",
    "    candidates = [start, *start.parents]\n",
    "    for base in candidates:\n",
    "        # Case 1: notebook opened from the demo directory\n",
    "        if (base / \"scripts\" / \"generate_superhero_datasets.py\").exists():\n",
    "            return base\n",
    "        # Case 2: notebook opened from repo root (or elsewhere)\n",
    "        demo = base / \"demos\" / \"pprl-superhero-example\"\n",
    "        if (demo / \"scripts\" / \"generate_superhero_datasets.py\").exists():\n",
    "            return demo\n",
    "    raise FileNotFoundError(\"Could not locate demos/pprl-superhero-example\")\n",
    "\n",
    "\n",
    "demo_dir = _find_demo_dir()\n",
    "scripts_dir = demo_dir / \"scripts\"\n",
    "datasets_dir = demo_dir / \"datasets\"\n",
    "outputs_dir = demo_dir / \"outputs\"\n",
    "\n",
    "datasets_dir.mkdir(parents=True, exist_ok=True)\n",
    "outputs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Demo directory: {demo_dir}\")\n",
    "\n",
    "# Demo-only secrets used by the in-notebook PySpark fast path.\n",
    "# The CLI script path uses the secrets embedded in the demo scripts instead.\n",
    "HASHING_SECRET = \"HashingKey\"\n",
    "ENCRYPTION_KEY = \"Secret-Encryption-Key-Goes-Here.\"  # Must be exactly 32 characters\n",
    "\n",
    "pyspark_available = False\n",
    "spark = None\n",
    "OpenTokenProcessor = None\n",
    "OpenTokenOverlapAnalyzer = None\n",
    "\n",
    "try:\n",
    "    from pyspark.sql import SparkSession  # type: ignore\n",
    "    from opentoken_pyspark import OpenTokenProcessor  # type: ignore\n",
    "    from opentoken_pyspark.overlap_analyzer import OpenTokenOverlapAnalyzer  # type: ignore\n",
    "\n",
    "    pyspark_available = True\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .appName(\"PPRL-Superhero-Demo\")\n",
    "        .master(\"local[*]\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "        .config(\"spark.driver.memory\", \"2g\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    print(\"✓ PySpark + OpenToken PySpark Bridge available (optional fast path)\")\n",
    "except Exception as e:\n",
    "    print(f\"PySpark bridge not available (will use demo scripts): {type(e).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18cddc",
   "metadata": {},
   "source": [
    "## 2. Generate Superhero Datasets\n",
    "\n",
    "Create two datasets (hospital and pharmacy) with a 40% overlap. The overlap represents patients that appear in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66767d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the data generation script\n",
    "result = subprocess.run(\n",
    "    [sys.executable, str(scripts_dir / \"generate_superhero_datasets.py\")],\n",
    "    cwd=str(demo_dir),\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    check=False,\n",
    " )\n",
    "\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print(f\"Error: {result.stderr}\")\n",
    "    raise RuntimeError(\"Dataset generation failed\")\n",
    "print(\"✓ Datasets generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3980ca",
   "metadata": {},
   "source": [
    "### Inspect the Generated Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643634a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display hospital dataset\n",
    "hospital_df = pd.read_csv(datasets_dir / 'hospital_superhero_data.csv')\n",
    "print(f\"Hospital Dataset: {len(hospital_df)} records\")\n",
    "print(hospital_df.head())\n",
    "print()\n",
    "\n",
    "# Load and display pharmacy dataset\n",
    "pharmacy_df = pd.read_csv(datasets_dir / 'pharmacy_superhero_data.csv')\n",
    "print(f\"Pharmacy Dataset: {len(pharmacy_df)} records\")\n",
    "print(pharmacy_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ebaa01",
   "metadata": {},
   "source": [
    "## 3. Tokenize the Datasets (in this notebook)\n",
    "\n",
    "Each organization tokenizes their data independently. In this notebook, we tokenize using the **OpenToken PySpark Bridge**, which applies:\n",
    "1. Normalization\n",
    "2. HMAC-SHA256 hashing for a deterministic fingerprint layer\n",
    "3. AES-256-GCM encryption (random IVs) for safe sharing\n",
    "\n",
    "**Important:** Both organizations must use the same hashing secret and encryption key (shared securely ahead of time), otherwise tokens won’t be comparable.\n",
    "\n",
    "**Note:** The end-to-end script (`run_end_to_end.sh`) runs a similar flow using the CLI scripts in this demo directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokenizing datasets...\")\n",
    "print()\n",
    "\n",
    "if pyspark_available and spark:\n",
    "    # Tokenize datasets using OpenToken PySpark Bridge\n",
    "    print(\"Using OpenToken PySpark Bridge...\")\n",
    "    print()\n",
    "\n",
    "    # Configuration (demo secrets only)\n",
    "    hashing_secret = HASHING_SECRET\n",
    "    encryption_key = ENCRYPTION_KEY\n",
    "\n",
    "    # Verify encryption key length\n",
    "    if len(encryption_key) != 32:\n",
    "        raise ValueError(f\"Encryption key must be exactly 32 characters, got {len(encryption_key)}\")\n",
    "\n",
    "    # Initialize the OpenToken processor\n",
    "    processor = OpenTokenProcessor(\n",
    "        hashing_secret=hashing_secret,\n",
    "        encryption_key=encryption_key,\n",
    "    )\n",
    "    print(\"✓ OpenToken Processor initialized\")\n",
    "\n",
    "    # Load hospital dataset into Spark\n",
    "    print(\"Loading hospital dataset...\")\n",
    "    hospital_spark_df = spark.read.csv(\n",
    "        str(datasets_dir / \"hospital_superhero_data.csv\"),\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "    )\n",
    "    print(f\"  Loaded {hospital_spark_df.count()} records\")\n",
    "\n",
    "    # Generate tokens for hospital dataset\n",
    "    print(\"Generating hospital tokens...\")\n",
    "    hospital_tokens_spark = processor.process_dataframe(hospital_spark_df)\n",
    "    hospital_token_count = hospital_tokens_spark.count()\n",
    "    print(f\"  Generated {hospital_token_count} tokens\")\n",
    "\n",
    "    # Save hospital tokens to CSV\n",
    "    hospital_tokens_spark.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\n",
    "        str(outputs_dir / \"hospital_tokens_temp\")\n",
    "    )\n",
    "\n",
    "    # Move the CSV file to the expected location\n",
    "    csv_file = glob.glob(str(outputs_dir / \"hospital_tokens_temp/*.csv\"))[0]\n",
    "    os.rename(csv_file, str(outputs_dir / \"hospital_tokens.csv\"))\n",
    "    shutil.rmtree(str(outputs_dir / \"hospital_tokens_temp\"))\n",
    "    print(\"✓ Hospital dataset tokenized\")\n",
    "\n",
    "    # Load pharmacy dataset into Spark\n",
    "    print(\"Loading pharmacy dataset...\")\n",
    "    pharmacy_spark_df = spark.read.csv(\n",
    "        str(datasets_dir / \"pharmacy_superhero_data.csv\"),\n",
    "        header=True,\n",
    "        inferSchema=True,\n",
    "    )\n",
    "    print(f\"  Loaded {pharmacy_spark_df.count()} records\")\n",
    "\n",
    "    # Generate tokens for pharmacy dataset\n",
    "    print(\"Generating pharmacy tokens...\")\n",
    "    pharmacy_tokens_spark = processor.process_dataframe(pharmacy_spark_df)\n",
    "    pharmacy_token_count = pharmacy_tokens_spark.count()\n",
    "    print(f\"  Generated {pharmacy_token_count} tokens\")\n",
    "\n",
    "    # Save pharmacy tokens to CSV\n",
    "    pharmacy_tokens_spark.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\n",
    "        str(outputs_dir / \"pharmacy_tokens_temp\")\n",
    "    )\n",
    "    csv_file = glob.glob(str(outputs_dir / \"pharmacy_tokens_temp/*.csv\"))[0]\n",
    "    os.rename(csv_file, str(outputs_dir / \"pharmacy_tokens.csv\"))\n",
    "    shutil.rmtree(str(outputs_dir / \"pharmacy_tokens_temp\"))\n",
    "    print(\"✓ Pharmacy dataset tokenized\")\n",
    "    print()\n",
    "    print(\"✓ Tokenization completed successfully using PySpark!\")\n",
    "else:\n",
    "    # Fall back to the standard demo scripts (Java CLI under the hood)\n",
    "    print(\"Using standard demo scripts:\")\n",
    "    print(\"  - scripts/tokenize_hospital.sh\")\n",
    "    print(\"  - scripts/tokenize_pharmacy.sh\")\n",
    "    print()\n",
    "\n",
    "    hospital_script = scripts_dir / \"tokenize_hospital.sh\"\n",
    "    pharmacy_script = scripts_dir / \"tokenize_pharmacy.sh\"\n",
    "\n",
    "    subprocess.run([\"chmod\", \"+x\", str(hospital_script)], cwd=str(demo_dir), check=False)\n",
    "    subprocess.run([\"chmod\", \"+x\", str(pharmacy_script)], cwd=str(demo_dir), check=False)\n",
    "\n",
    "    result = subprocess.run(\n",
    "        [str(hospital_script)],\n",
    "        cwd=str(demo_dir),\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=False,\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "        raise RuntimeError(\"Hospital tokenization failed\")\n",
    "\n",
    "    result = subprocess.run(\n",
    "        [str(pharmacy_script)],\n",
    "        cwd=str(demo_dir),\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=False,\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "        raise RuntimeError(\"Pharmacy tokenization failed\")\n",
    "\n",
    "    print(\"✓ Tokenization completed successfully using the demo scripts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefeb79d",
   "metadata": {},
   "source": [
    "### Inspect Tokenized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f011cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized hospital data\n",
    "hospital_tokens = pd.read_csv(outputs_dir / 'hospital_tokens.csv')\n",
    "print(f\"Hospital Tokens: {len(hospital_tokens)} token rows (5 per patient)\")\n",
    "print(hospital_tokens.head(10))\n",
    "print()\n",
    "\n",
    "# Load tokenized pharmacy data\n",
    "pharmacy_tokens = pd.read_csv(outputs_dir / 'pharmacy_tokens.csv')\n",
    "print(f\"Pharmacy Tokens: {len(pharmacy_tokens)} token rows (5 per patient)\")\n",
    "print(pharmacy_tokens.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4935eeaf",
   "metadata": {},
   "source": [
    "## 4. Decrypt Tokens and Perform Overlap Analysis\n",
    "\n",
    "To compare tokens across independently tokenized datasets:\n",
    "1. **Decrypt** the encrypted tokens to reveal the underlying HMAC-SHA256 fingerprints\n",
    "2. **Compare** those fingerprints to find matching records\n",
    "\n",
    "**Why decryption is needed:** OpenToken uses random IVs for encryption, so even identical patients produce different encrypted token strings. Decryption reveals the deterministic fingerprint layer that can be compared for equality.\n",
    "\n",
    "**Who runs this step (in this demo):** assume the **hospital** runs the overlap analysis step in a trusted environment (it needs access to the decryption key).\n",
    "\n",
    "**Match policy reminder:** the set of token IDs you require to match (e.g., T1–T5, or a smaller subset) is a policy choice the parties agree on up front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c8ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performing overlap analysis...\")\n",
    "print()\n",
    "\n",
    "# Use the PySpark bridge if available; otherwise use the standard demo script.\n",
    "if pyspark_available and spark:\n",
    "    print(\"Using OpenToken PySpark Bridge...\")\n",
    "    print()\n",
    "\n",
    "    try:\n",
    "        # Load tokenized data into Spark DataFrames\n",
    "        hospital_tokens_spark = spark.read.csv(\n",
    "            str(outputs_dir / 'hospital_tokens.csv'),\n",
    "            header=True,\n",
    "            inferSchema=True,\n",
    "        )\n",
    "        pharmacy_tokens_spark = spark.read.csv(\n",
    "            str(outputs_dir / 'pharmacy_tokens.csv'),\n",
    "            header=True,\n",
    "            inferSchema=True,\n",
    "        )\n",
    "\n",
    "        print(f\"Hospital tokens loaded: {hospital_tokens_spark.count()} rows\")\n",
    "        print(f\"Pharmacy tokens loaded: {pharmacy_tokens_spark.count()} rows\")\n",
    "        print()\n",
    "\n",
    "        # Initialize the overlap analyzer with the encryption key\n",
    "        encryption_key = \"Secret-Encryption-Key-Goes-Here.\"  # Same key used for tokenization\n",
    "        analyzer = OpenTokenOverlapAnalyzer(encryption_key)\n",
    "\n",
    "        print(\"Decrypting and matching tokens...\")\n",
    "\n",
    "        # matching_rules specifies which token IDs must ALL match\n",
    "        results = analyzer.analyze_overlap(\n",
    "            hospital_tokens_spark,\n",
    "            pharmacy_tokens_spark,\n",
    "            matching_rules=[\"T1\", \"T2\", \"T3\", \"T4\", \"T5\"],\n",
    "            dataset1_name=\"Hospital\",\n",
    "            dataset2_name=\"Pharmacy\",\n",
    "        )\n",
    "\n",
    "        print(\"✓ Overlap analysis completed using OpenToken PySpark Bridge!\")\n",
    "        print()\n",
    "\n",
    "        # Display results\n",
    "        print(\"=\" * 70)\n",
    "        print(\"OVERLAP ANALYSIS RESULTS\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Total Hospital records: {results['total_records_dataset1']}\")\n",
    "        print(f\"Total Pharmacy records: {results['total_records_dataset2']}\")\n",
    "        print(f\"Matching Hospital records: {results['matching_records_dataset1']}\")\n",
    "        print(f\"Matching Pharmacy records: {results['matching_records_dataset2']}\")\n",
    "        print(f\"Overlap percentage: {results['overlap_percentage']:.1f}%\")\n",
    "        print(f\"Records unique to Hospital: {results['unique_to_dataset1']}\")\n",
    "        print(f\"Records unique to Pharmacy: {results['unique_to_dataset2']}\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "\n",
    "        # Get the matches DataFrame\n",
    "        matches_df_spark = results['matches']\n",
    "        print(\"Sample matches (first 10):\")\n",
    "        matches_df_spark.show(10, truncate=False)\n",
    "        print()\n",
    "\n",
    "        # Save results to CSV\n",
    "        matches_df_spark.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\n",
    "            str(outputs_dir / 'matching_records_temp')\n",
    "        )\n",
    "        import glob\n",
    "        import shutil\n",
    "\n",
    "        csv_file = glob.glob(str(outputs_dir / 'matching_records_temp/*.csv'))[0]\n",
    "        shutil.copy(csv_file, str(outputs_dir / 'matching_records.csv'))\n",
    "        shutil.rmtree(str(outputs_dir / 'matching_records_temp'))\n",
    "        print(\"✓ Results saved to outputs/matching_records.csv\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error during PySpark analysis: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        print()\n",
    "        print(\"Falling back to the standard demo overlap script...\")\n",
    "        pyspark_available = False  # force fallback below\n",
    "\n",
    "\n",
    "if not pyspark_available:\n",
    "    print(\"Using standard demo script: scripts/analyze_overlap.py\")\n",
    "    analyze_script = scripts_dir / 'analyze_overlap.py'\n",
    "    result = subprocess.run(\n",
    "        ['python', str(analyze_script)],\n",
    "        cwd=str(demo_dir),\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "    )\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "        raise RuntimeError(\"Overlap analysis failed\")\n",
    "    print(\"✓ Overlap analysis completed successfully using the demo script\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d2c41",
   "metadata": {},
   "source": [
    "### View Matching Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display matching results\n",
    "matches_df = pd.read_csv(outputs_dir / 'matching_records.csv')\n",
    "\n",
    "print(f\"Total Matching Pairs: {len(matches_df)}\")\n",
    "print()\n",
    "print(\"First 10 matching records:\")\n",
    "print(matches_df.head(10))\n",
    "print()\n",
    "\n",
    "# Summary statistics\n",
    "hospital_count = len(hospital_df)\n",
    "pharmacy_count = len(pharmacy_df)\n",
    "unique_hospital_matches = matches_df['Hospital_RecordId'].nunique()\n",
    "unique_pharmacy_matches = matches_df['Pharmacy_RecordId'].nunique()\n",
    "\n",
    "print(\"Summary Statistics:\")\n",
    "print(f\"- Hospital records with matches: {unique_hospital_matches} out of {hospital_count}\")\n",
    "print(f\"- Pharmacy records with matches: {unique_pharmacy_matches} out of {pharmacy_count}\")\n",
    "print(f\"- Overlap percentage (hospital): {(unique_hospital_matches / hospital_count * 100):.1f}%\")\n",
    "print(f\"- Overlap percentage (pharmacy): {(unique_pharmacy_matches / pharmacy_count * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa68179",
   "metadata": {},
   "source": [
    "## 5. Alternative Analysis: Relaxed Matching Rules\n",
    "\n",
    "Let's compare the results when we relax the matching requirements. Instead of requiring all 5 tokens (T1-T5) to match, we'll only require T1, T2, T3, and T5 to match (excluding T4).\n",
    "\n",
    "**Use case**: This can be useful when one attribute (like postal code in T4) might have data quality issues or when you want to cast a wider net for potential matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative overlap analysis with relaxed matching rules (requires PySpark bridge).\n",
    "print(\"Performing alternative overlap analysis (T1, T2, T3, T5 only)...\")\n",
    "print()\n",
    "\n",
    "if not (pyspark_available and spark):\n",
    "    print(\"Skipping: PySpark bridge not available in this environment.\")\n",
    "elif \"results\" not in globals():\n",
    "    print(\"Skipping: run the main overlap analysis cell first.\")\n",
    "else:\n",
    "    try:\n",
    "        # Load tokenized data into Spark DataFrames\n",
    "        hospital_tokens_spark_alt = spark.read.csv(\n",
    "            str(outputs_dir / \"hospital_tokens.csv\"),\n",
    "            header=True,\n",
    "            inferSchema=True,\n",
    "        )\n",
    "        pharmacy_tokens_spark_alt = spark.read.csv(\n",
    "            str(outputs_dir / \"pharmacy_tokens.csv\"),\n",
    "            header=True,\n",
    "            inferSchema=True,\n",
    "        )\n",
    "\n",
    "        # Initialize the overlap analyzer\n",
    "        analyzer_alt = OpenTokenOverlapAnalyzer(ENCRYPTION_KEY)\n",
    "\n",
    "        print(\"Decrypting and matching tokens (excluding T4)...\")\n",
    "\n",
    "        # Analyze overlap with relaxed rules - only T1, T2, T3, T5\n",
    "        results_alt = analyzer_alt.analyze_overlap(\n",
    "            hospital_tokens_spark_alt,\n",
    "            pharmacy_tokens_spark_alt,\n",
    "            matching_rules=[\"T1\", \"T2\", \"T3\", \"T5\"],  # Excluding T4\n",
    "            dataset1_name=\"Hospital\",\n",
    "            dataset2_name=\"Pharmacy\",\n",
    "        )\n",
    "\n",
    "        print(\"✓ Alternative overlap analysis completed!\")\n",
    "        print()\n",
    "\n",
    "        # Compare with original results\n",
    "        print(\"=\" * 70)\n",
    "        print(\"COMPARISON: All 5 Tokens vs. 4 Tokens (excluding T4)\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Strict matching (T1-T5): {results['matching_records_dataset1']} hospital matches\")\n",
    "        print(f\"Relaxed matching (T1,T2,T3,T5): {results_alt['matching_records_dataset1']} hospital matches\")\n",
    "        print(f\"Additional matches found: {results_alt['matching_records_dataset1'] - results['matching_records_dataset1']}\")\n",
    "        print(\"=\" * 70)\n",
    "        print()\n",
    "\n",
    "        # Save alternative results\n",
    "        matches_df_spark_alt = results_alt[\"matches\"]\n",
    "        matches_df_spark_alt.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\n",
    "            str(outputs_dir / \"matching_records_alt_temp\")\n",
    "        )\n",
    "        csv_file_alt = glob.glob(str(outputs_dir / \"matching_records_alt_temp/*.csv\"))[0]\n",
    "        shutil.copy(csv_file_alt, str(outputs_dir / \"matching_records_alt.csv\"))\n",
    "        shutil.rmtree(str(outputs_dir / \"matching_records_alt_temp\"))\n",
    "        print(\"✓ Alternative results saved to outputs/matching_records_alt.csv\")\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error during alternative analysis: {e}\")\n",
    "        print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f8c62",
   "metadata": {},
   "source": [
    "### Interpreting the Alternative Analysis\n",
    "\n",
    "The alternative analysis uses only 4 tokens (T1, T2, T3, T5) instead of all 5:\n",
    "- **T1**: FirstName + LastName + Sex + BirthDate\n",
    "- **T2**: FirstName + LastName + PostalCode\n",
    "- **T3**: FirstName + LastName + SocialSecurityNumber\n",
    "- **T4**: ❌ *EXCLUDED* - BirthDate + Sex + PostalCode\n",
    "- **T5**: BirthDate + Sex + SocialSecurityNumber\n",
    "\n",
    "By excluding T4, we're being more lenient about postal code consistency. This might find additional matches where:\n",
    "- Postal codes have typos or formatting differences\n",
    "- People have moved between visits\n",
    "- Data entry errors occurred\n",
    "\n",
    "However, this also increases the risk of false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8deb8",
   "metadata": {},
   "source": [
    "## 6. Understand the Results\n",
    "\n",
    "Let's look at what a match actually means by examining some matched records in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24fa176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample matched record\n",
    "if len(matches_df) > 0:\n",
    "    sample_match = matches_df.iloc[0]\n",
    "    hospital_record_id = sample_match['Hospital_RecordId']\n",
    "    pharmacy_record_id = sample_match['Pharmacy_RecordId']\n",
    "    \n",
    "    # Get the original records\n",
    "    hospital_match = hospital_df[hospital_df['RecordId'] == hospital_record_id]\n",
    "    pharmacy_match = pharmacy_df[pharmacy_df['RecordId'] == pharmacy_record_id]\n",
    "    \n",
    "    if len(hospital_match) == 0 or len(pharmacy_match) == 0:\n",
    "        print(f\"Warning: Could not find matching records in original datasets\")\n",
    "        print(f\"Hospital RecordId {hospital_record_id} found: {len(hospital_match) > 0}\")\n",
    "        print(f\"Pharmacy RecordId {pharmacy_record_id} found: {len(pharmacy_match) > 0}\")\n",
    "    else:\n",
    "        hospital_record = hospital_match.iloc[0]\n",
    "        pharmacy_record = pharmacy_match.iloc[0]\n",
    "        \n",
    "        print(\"Sample Match:\")\n",
    "        print(f\"Hospital Record ID: {hospital_record_id}\")\n",
    "        print(f\"Hospital Patient: {hospital_record['FirstName']} {hospital_record['LastName']}\")\n",
    "        print(f\"DOB: {hospital_record['BirthDate']}, SSN: {hospital_record['SocialSecurityNumber']}\")\n",
    "        print()\n",
    "        print(f\"Pharmacy Record ID: {pharmacy_record_id}\")\n",
    "        print(f\"Pharmacy Patient: {pharmacy_record['FirstName']} {pharmacy_record['LastName']}\")\n",
    "        print(f\"DOB: {pharmacy_record['BirthDate']}, SSN: {pharmacy_record['SocialSecurityNumber']}\")\n",
    "        print()\n",
    "        print(\"✓ All 5 tokens matched, confirming this is the same patient!\")\n",
    "else:\n",
    "    print(\"No matches found. This could happen if:\")\n",
    "    print(\"- Different hashing/encryption keys were used\")\n",
    "    print(\"- Data validation rejected records with invalid attributes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef457335",
   "metadata": {},
   "source": [
    "## 7. Advanced PySpark Transformations (Optional)\n",
    "\n",
    "If PySpark is available, we can perform distributed transformations on the tokenized data for large-scale analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark-based transformations for distributed processing\n",
    "if pyspark_available and spark:\n",
    "    try:\n",
    "        from pyspark.sql.functions import col, count as spark_count\n",
    "        \n",
    "        print(\"Performing distributed token analysis with PySpark...\")\n",
    "        print()\n",
    "        \n",
    "        # Load the tokenized data (all columns as strings to avoid type inference issues)\n",
    "        hospital_tokens_spark = spark.read.csv(\n",
    "            str(outputs_dir / 'hospital_tokens.csv'),\n",
    "            header=True,\n",
    "            inferSchema=False  # Keep all as strings\n",
    "        )\n",
    "        pharmacy_tokens_spark = spark.read.csv(\n",
    "            str(outputs_dir / 'pharmacy_tokens.csv'),\n",
    "            header=True,\n",
    "            inferSchema=False  # Keep all as strings\n",
    "        )\n",
    "        \n",
    "        # Analyze token distribution in hospital dataset\n",
    "        print(\"Hospital Token Distribution:\")\n",
    "        hospital_tokens_spark.groupBy(\"RuleId\").agg(spark_count(\"*\").alias(\"count\")).orderBy(\"RuleId\").show()\n",
    "        print()\n",
    "        \n",
    "        # Analyze token distribution in pharmacy dataset\n",
    "        print(\"Pharmacy Token Distribution:\")\n",
    "        pharmacy_tokens_spark.groupBy(\"RuleId\").agg(spark_count(\"*\").alias(\"count\")).orderBy(\"RuleId\").show()\n",
    "        print()\n",
    "        \n",
    "        # Count unique records\n",
    "        hospital_unique = hospital_tokens_spark.select(\"RecordId\").distinct().count()\n",
    "        pharmacy_unique = pharmacy_tokens_spark.select(\"RecordId\").distinct().count()\n",
    "        print(f\"Unique records - Hospital: {hospital_unique}, Pharmacy: {pharmacy_unique}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Note: Advanced transformations not available - {type(e).__name__}\")\n",
    "        print(\"This is optional and does not affect the core PPRL workflow.\")\n",
    "else:\n",
    "    print(\"PySpark not available for advanced transformations.\")\n",
    "    print(\"Core PPRL analysis completed successfully using pandas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35422623",
   "metadata": {},
   "source": [
    "## 8. Privacy and Security Summary\n",
    "\n",
    "This demonstration shows how OpenToken enables privacy-preserving record linkage:\n",
    "\n",
    "### What was protected:\n",
    "- ✓ Raw patient data (names, SSNs, birthdates) was never shared between organizations\n",
    "- ✓ HMAC-SHA256 hashes cannot be reversed to recover original data\n",
    "- ✓ Encryption key controls who can decrypt and perform linkage\n",
    "\n",
    "### What was shared:\n",
    "- • Encrypted tokens for secure transmission\n",
    "- • Matching statistics showing overlap counts\n",
    "- • Metadata with summary information (not raw data)\n",
    "\n",
    "### Key security principles:\n",
    "1. **Strong Encryption**: AES-256-GCM with random IVs prevents pattern analysis\n",
    "2. **Key Management**: Secure sharing and storage of encryption/hashing keys\n",
    "3. **Deterministic Hashing**: HMAC-SHA256 enables comparison without raw data\n",
    "4. **Access Control**: Only authorized parties can decrypt tokens\n",
    "\n",
    "### PySpark Bridge Benefits:\n",
    "- **Distributed Processing**: Handle large datasets across multiple nodes\n",
    "- **Parallel Decryption**: Efficiently decrypt millions of tokens\n",
    "- **Scalable Analysis**: Perform overlap analysis on enterprise-scale data\n",
    "- **Integration**: Native Spark SQL for additional transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f42e1d",
   "metadata": {},
   "source": [
    "## 9. Customization Examples\n",
    "\n",
    "You can customize this demo by modifying the scripts:\n",
    "\n",
    "### Change dataset size and overlap:\n",
    "Edit `scripts/generate_superhero_datasets.py`:\n",
    "```python\n",
    "num_hospital = 500  # Different size\n",
    "num_pharmacy = 600\n",
    "overlap_percentage = 0.50  # 50% overlap instead of 40%\n",
    "```\n",
    "\n",
    "### Use different encryption keys:\n",
    "Edit `scripts/tokenize_datasets.sh`:\n",
    "```bash\n",
    "HASH_KEY=\"YourCustomHashingKey\"\n",
    "ENCRYPTION_KEY=\"YourCustomEncryptionKey-32\"\n",
    "```\n",
    "\n",
    "**Important**: Both organizations must use the same keys for tokens to match!\n",
    "\n",
    "### Scale with PySpark:\n",
    "For large datasets, ensure PySpark is installed:\n",
    "```bash\n",
    "pip install pyspark opentoken-pyspark\n",
    "```\n",
    "\n",
    "The notebook will automatically use distributed processing if available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a16e4",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "This PPRL demo can be adapted for:\n",
    "- Healthcare: Hospital-to-hospital patient matching\n",
    "- Insurance: Claims linkage across providers\n",
    "- Research: Multi-site study participant matching\n",
    "- Government: Cross-agency identity resolution\n",
    "- Financial Services: Anti-fraud systems\n",
    "\n",
    "### With PySpark Bridge:\n",
    "- Scale to petabyte-level datasets\n",
    "- Distribute tokenization across clusters\n",
    "- Parallel overlap analysis\n",
    "- Real-time record linkage pipelines\n",
    "\n",
    "For more information, see the [README.md](./README.md) in this directory and the [main OpenToken documentation](../../README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca8862c",
   "metadata": {},
   "source": [
    "<!-- notebook-edit-test -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
