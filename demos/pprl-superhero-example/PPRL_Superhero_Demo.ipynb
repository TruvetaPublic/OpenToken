{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d416c681",
   "metadata": {},
   "source": [
    "# Privacy-Preserving Record Linkage (PPRL) - Superhero Demo with PySpark\n",
    "\n",
    "This notebook demonstrates how OpenToken enables privacy-preserving record linkage between two organizations without sharing sensitive patient data.\n",
    "\n",
    "**Scenario**: Super Hero Hospital and Super Hero Pharmacy want to link patient records for care coordination while protecting patient privacy.\n",
    "\n",
    "**PySpark Bridge**: This notebook uses the OpenToken PySpark bridge for distributed overlap analysis and advanced transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83162fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up paths\n",
    "demo_dir = Path.cwd()\n",
    "scripts_dir = demo_dir / 'scripts'\n",
    "datasets_dir = demo_dir / 'datasets'\n",
    "outputs_dir = demo_dir / 'outputs'\n",
    "\n",
    "print(f\"Demo directory: {demo_dir}\")\n",
    "print(f\"Scripts directory: {scripts_dir}\")\n",
    "print(f\"Datasets directory: {datasets_dir}\")\n",
    "print(f\"Outputs directory: {outputs_dir}\")\n",
    "print()\n",
    "\n",
    "# Try to import PySpark components\n",
    "pyspark_available = False\n",
    "spark = None\n",
    "\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    print(\"✓ PySpark available\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠ PySpark not available: {e}\")\n",
    "    print(\"  Install with: pip install pyspark\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    from opentoken_pyspark import OpenTokenProcessor\n",
    "    from opentoken_pyspark.overlap_analyzer import OpenTokenOverlapAnalyzer\n",
    "    print(\"✓ OpenToken PySpark Bridge available\")\n",
    "    pyspark_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"⚠ OpenToken PySpark Bridge not available: {e}\")\n",
    "    print(\"  Install with: pip install opentoken-pyspark\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Initialize Spark Session\n",
    "if pyspark_available:\n",
    "    try:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"PPRL-Superhero-Demo\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "            .config(\"spark.driver.memory\", \"2g\") \\\n",
    "            .getOrCreate()\n",
    "        spark.sparkContext.setLogLevel(\"WARN\")\n",
    "        print(\"✓ Spark Session initialized successfully\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Could not initialize Spark: {e}\")\n",
    "        print(\"Make sure PySpark is properly installed\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18cddc",
   "metadata": {},
   "source": [
    "## 2. Generate Superhero Datasets\n",
    "\n",
    "Create two datasets (hospital and pharmacy) with a 40% overlap. The overlap represents patients that appear in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66767d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the data generation script\n",
    "result = subprocess.run(\n",
    "    ['python', str(scripts_dir / 'generate_superhero_datasets.py')],\n",
    "    cwd=str(demo_dir),\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(result.stdout)\n",
    "if result.returncode != 0:\n",
    "    print(f\"Error: {result.stderr}\")\n",
    "else:\n",
    "    print(\"✓ Datasets generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3980ca",
   "metadata": {},
   "source": [
    "### Inspect the Generated Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643634a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display hospital dataset\n",
    "hospital_df = pd.read_csv(datasets_dir / 'hospital_superhero_data.csv')\n",
    "print(f\"Hospital Dataset: {len(hospital_df)} records\")\n",
    "print(hospital_df.head())\n",
    "print()\n",
    "\n",
    "# Load and display pharmacy dataset\n",
    "pharmacy_df = pd.read_csv(datasets_dir / 'pharmacy_superhero_data.csv')\n",
    "print(f\"Pharmacy Dataset: {len(pharmacy_df)} records\")\n",
    "print(pharmacy_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ebaa01",
   "metadata": {},
   "source": [
    "## 3. Tokenize the Datasets with PySpark\n",
    "\n",
    "Each organization tokenizes their data independently using the OpenToken PySpark Bridge. This applies:\n",
    "1. HMAC-SHA256 hashing for deterministic tokens\n",
    "2. AES-256-GCM encryption for secure transmission\n",
    "\n",
    "**PySpark Integration**: Uses distributed processing to tokenize datasets in parallel across the Spark cluster.\n",
    "\n",
    "**Important**: Both organizations use the same hashing and encryption keys to enable later comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize datasets using OpenToken PySpark Bridge\n",
    "print(\"Tokenizing datasets with OpenToken PySpark Bridge...\")\n",
    "print()\n",
    "\n",
    "# Configuration\n",
    "hash_key = \"HashingKey\"\n",
    "encryption_key = \"Secret-Encryption-Key-Goes-Here.\"\n",
    "\n",
    "# Verify encryption key length\n",
    "if len(encryption_key) != 32:\n",
    "    raise ValueError(f\"Encryption key must be exactly 32 characters, got {len(encryption_key)}\")\n",
    "\n",
    "# Initialize the OpenToken processor\n",
    "processor = OpenTokenProcessor(\n",
    "    hashing_secret=hash_key,\n",
    "    encryption_key=encryption_key\n",
    ")\n",
    "print(\"✓ OpenToken Processor initialized\")\n",
    "\n",
    "# Load hospital dataset into Spark\n",
    "print(\"Loading hospital dataset...\")\n",
    "hospital_spark_df = spark.read.csv(\n",
    "    str(datasets_dir / 'hospital_superhero_data.csv'),\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "print(f\"  Loaded {hospital_spark_df.count()} records\")\n",
    "\n",
    "# Generate tokens for hospital dataset\n",
    "print(\"Generating hospital tokens...\")\n",
    "hospital_tokens_spark = processor.process_dataframe(hospital_spark_df)\n",
    "hospital_token_count = hospital_tokens_spark.count()\n",
    "print(f\"  Generated {hospital_token_count} tokens\")\n",
    "\n",
    "# Save hospital tokens to CSV\n",
    "hospital_tokens_spark.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\n",
    "    str(outputs_dir / 'hospital_tokens_temp')\n",
    ")\n",
    "# Move the CSV file to the expected location\n",
    "import glob\n",
    "csv_file = glob.glob(str(outputs_dir / 'hospital_tokens_temp/*.csv'))[0]\n",
    "os.rename(csv_file, str(outputs_dir / 'hospital_tokens.csv'))\n",
    "# Clean up temp directory\n",
    "import shutil\n",
    "shutil.rmtree(str(outputs_dir / 'hospital_tokens_temp'))\n",
    "print(\"✓ Hospital dataset tokenized\")\n",
    "\n",
    "# Load pharmacy dataset into Spark\n",
    "print(\"Loading pharmacy dataset...\")\n",
    "pharmacy_spark_df = spark.read.csv(\n",
    "    str(datasets_dir / 'pharmacy_superhero_data.csv'),\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "print(f\"  Loaded {pharmacy_spark_df.count()} records\")\n",
    "\n",
    "# Generate tokens for pharmacy dataset\n",
    "print(\"Generating pharmacy tokens...\")\n",
    "pharmacy_tokens_spark = processor.process_dataframe(pharmacy_spark_df)\n",
    "pharmacy_token_count = pharmacy_tokens_spark.count()\n",
    "print(f\"  Generated {pharmacy_token_count} tokens\")\n",
    "\n",
    "# Save pharmacy tokens to CSV\n",
    "pharmacy_tokens_spark.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\n",
    "    str(outputs_dir / 'pharmacy_tokens_temp')\n",
    ")\n",
    "# Move the CSV file to the expected location\n",
    "csv_file = glob.glob(str(outputs_dir / 'pharmacy_tokens_temp/*.csv'))[0]\n",
    "os.rename(csv_file, str(outputs_dir / 'pharmacy_tokens.csv'))\n",
    "# Clean up temp directory\n",
    "shutil.rmtree(str(outputs_dir / 'pharmacy_tokens_temp'))\n",
    "print(\"✓ Pharmacy dataset tokenized\")\n",
    "print()\n",
    "print(\"✓ Tokenization completed successfully using PySpark!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefeb79d",
   "metadata": {},
   "source": [
    "### Inspect Tokenized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f011cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized hospital data\n",
    "hospital_tokens = pd.read_csv(outputs_dir / 'hospital_tokens.csv')\n",
    "print(f\"Hospital Tokens: {len(hospital_tokens)} token rows (5 per patient)\")\n",
    "print(hospital_tokens.head(10))\n",
    "print()\n",
    "\n",
    "# Load tokenized pharmacy data\n",
    "pharmacy_tokens = pd.read_csv(outputs_dir / 'pharmacy_tokens.csv')\n",
    "print(f\"Pharmacy Tokens: {len(pharmacy_tokens)} token rows (5 per patient)\")\n",
    "print(pharmacy_tokens.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4935eeaf",
   "metadata": {},
   "source": [
    "## 4. Decrypt Tokens and Perform Overlap Analysis\n",
    "\n",
    "To compare tokens across independently tokenized datasets:\n",
    "1. **Decrypt** the encrypted tokens to reveal the underlying HMAC-SHA256 hashes\n",
    "2. **Compare** the decrypted hashes to find matching records\n",
    "\n",
    "**Why decryption is needed**: OpenToken uses random IVs for encryption, so even identical patients produce different encrypted tokens. Decryption reveals the deterministic hash layer that can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c8ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OpenToken PySpark Bridge for overlap analysis\n",
    "print(\"Performing overlap analysis with OpenToken PySpark Bridge...\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Load tokenized data into Spark DataFrames\n",
    "    hospital_tokens_spark = spark.read.csv(\n",
    "        str(outputs_dir / 'hospital_tokens.csv'),\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    pharmacy_tokens_spark = spark.read.csv(\n",
    "        str(outputs_dir / 'pharmacy_tokens.csv'),\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Hospital tokens loaded: {hospital_tokens_spark.count()} rows\")\n",
    "    print(f\"Pharmacy tokens loaded: {pharmacy_tokens_spark.count()} rows\")\n",
    "    print()\n",
    "    \n",
    "    # Initialize the overlap analyzer with the encryption key\n",
    "    encryption_key = \"Secret-Encryption-Key-Goes-Here.\"  # Same key used for tokenization\n",
    "    analyzer = OpenTokenOverlapAnalyzer(encryption_key)\n",
    "    \n",
    "    print(\"Decrypting and matching tokens...\")\n",
    "    \n",
    "    # Analyze overlap using PySpark Bridge\n",
    "    # The matching_rules parameter specifies which tokens must ALL match (T1-T5)\n",
    "    results = analyzer.analyze_overlap(\n",
    "        hospital_tokens_spark,\n",
    "        pharmacy_tokens_spark,\n",
    "        matching_rules=[\"T1\", \"T2\", \"T3\", \"T4\", \"T5\"],\n",
    "        dataset1_name=\"Hospital\",\n",
    "        dataset2_name=\"Pharmacy\"\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Overlap analysis completed using OpenToken PySpark Bridge!\")\n",
    "    print()\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=\" * 70)\n",
    "    print(\"OVERLAP ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total Hospital records: {results['total_records_dataset1']}\")\n",
    "    print(f\"Total Pharmacy records: {results['total_records_dataset2']}\")\n",
    "    print(f\"Matching Hospital records: {results['matching_records_dataset1']}\")\n",
    "    print(f\"Matching Pharmacy records: {results['matching_records_dataset2']}\")\n",
    "    print(f\"Overlap percentage: {results['overlap_percentage']:.1f}%\")\n",
    "    print(f\"Records unique to Hospital: {results['unique_to_dataset1']}\")\n",
    "    print(f\"Records unique to Pharmacy: {results['unique_to_dataset2']}\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    \n",
    "    # Get the matches DataFrame\n",
    "    matches_df_spark = results['matches']\n",
    "    print(\"Sample matches (first 10):\")\n",
    "    matches_df_spark.show(10, truncate=False)\n",
    "    print()\n",
    "    \n",
    "    # Save results to CSV\n",
    "    matches_df_spark.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\n",
    "        str(outputs_dir / 'matching_records_temp')\n",
    "    )\n",
    "    # Move the CSV file to the expected location\n",
    "    import glob\n",
    "    csv_file = glob.glob(str(outputs_dir / 'matching_records_temp/*.csv'))[0]\n",
    "    import shutil\n",
    "    shutil.copy(csv_file, str(outputs_dir / 'matching_records.csv'))\n",
    "    shutil.rmtree(str(outputs_dir / 'matching_records_temp'))\n",
    "    print(\"✓ Results saved to outputs/matching_records.csv\")\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error during PySpark analysis: {e}\")\n",
    "    print(traceback.format_exc())\n",
    "    print()\n",
    "    print(\"Falling back to pandas-based analysis...\")\n",
    "    \n",
    "    # Fallback to pandas-based analysis\n",
    "    print(\"Using pandas-based overlap analysis...\")\n",
    "    print()\n",
    "    \n",
    "    # Make the overlap analysis script executable\n",
    "    analyze_script = scripts_dir / 'analyze_overlap.py'\n",
    "\n",
    "    # Run overlap analysis\n",
    "    result = subprocess.run(\n",
    "        ['python', str(analyze_script)],\n",
    "        cwd=str(demo_dir),\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "\n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error: {result.stderr}\")\n",
    "    else:\n",
    "        print(\"✓ Overlap analysis completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d2c41",
   "metadata": {},
   "source": [
    "### View Matching Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7db44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display matching results\n",
    "matches_df = pd.read_csv(outputs_dir / 'matching_records.csv')\n",
    "\n",
    "print(f\"Total Matching Pairs: {len(matches_df)}\")\n",
    "print()\n",
    "print(\"First 10 matching records:\")\n",
    "print(matches_df.head(10))\n",
    "print()\n",
    "\n",
    "# Summary statistics\n",
    "hospital_count = len(hospital_df)\n",
    "pharmacy_count = len(pharmacy_df)\n",
    "unique_hospital_matches = matches_df['Hospital_RecordId'].nunique()\n",
    "unique_pharmacy_matches = matches_df['Pharmacy_RecordId'].nunique()\n",
    "\n",
    "print(\"Summary Statistics:\")\n",
    "print(f\"- Hospital records with matches: {unique_hospital_matches} out of {hospital_count}\")\n",
    "print(f\"- Pharmacy records with matches: {unique_pharmacy_matches} out of {pharmacy_count}\")\n",
    "print(f\"- Overlap percentage (hospital): {(unique_hospital_matches / hospital_count * 100):.1f}%\")\n",
    "print(f\"- Overlap percentage (pharmacy): {(unique_pharmacy_matches / pharmacy_count * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa68179",
   "metadata": {},
   "source": [
    "## 5. Alternative Analysis: Relaxed Matching Rules\n",
    "\n",
    "Let's compare the results when we relax the matching requirements. Instead of requiring all 5 tokens (T1-T5) to match, we'll only require T1, T2, T3, and T5 to match (excluding T4).\n",
    "\n",
    "**Use case**: This can be useful when one attribute (like postal code in T4) might have data quality issues or when you want to cast a wider net for potential matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run alternative overlap analysis with relaxed matching rules (T1, T2, T3, T5 only)\n",
    "print(\"Performing alternative overlap analysis (T1, T2, T3, T5 only)...\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Load tokenized data into Spark DataFrames\n",
    "    hospital_tokens_spark_alt = spark.read.csv(\n",
    "        str(outputs_dir / 'hospital_tokens.csv'),\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    pharmacy_tokens_spark_alt = spark.read.csv(\n",
    "        str(outputs_dir / 'pharmacy_tokens.csv'),\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    \n",
    "    # Initialize the overlap analyzer\n",
    "    analyzer_alt = OpenTokenOverlapAnalyzer(encryption_key)\n",
    "    \n",
    "    print(\"Decrypting and matching tokens (excluding T4)...\")\n",
    "    \n",
    "    # Analyze overlap with relaxed rules - only T1, T2, T3, T5\n",
    "    results_alt = analyzer_alt.analyze_overlap(\n",
    "        hospital_tokens_spark_alt,\n",
    "        pharmacy_tokens_spark_alt,\n",
    "        matching_rules=[\"T1\", \"T2\", \"T3\", \"T5\"],  # Excluding T4 (Postal Code)\n",
    "        dataset1_name=\"Hospital\",\n",
    "        dataset2_name=\"Pharmacy\"\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Alternative overlap analysis completed!\")\n",
    "    print()\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ALTERNATIVE ANALYSIS RESULTS (T1, T2, T3, T5 only)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total Hospital records: {results_alt['total_records_dataset1']}\")\n",
    "    print(f\"Total Pharmacy records: {results_alt['total_records_dataset2']}\")\n",
    "    print(f\"Matching Hospital records: {results_alt['matching_records_dataset1']}\")\n",
    "    print(f\"Matching Pharmacy records: {results_alt['matching_records_dataset2']}\")\n",
    "    print(f\"Overlap percentage: {results_alt['overlap_percentage']:.1f}%\")\n",
    "    print(f\"Records unique to Hospital: {results_alt['unique_to_dataset1']}\")\n",
    "    print(f\"Records unique to Pharmacy: {results_alt['unique_to_dataset2']}\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    \n",
    "    # Compare with original results\n",
    "    print(\"=\" * 70)\n",
    "    print(\"COMPARISON: All 5 Tokens vs. 4 Tokens (excluding T4)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Strict matching (T1-T5): {results['matching_records_dataset1']} hospital matches\")\n",
    "    print(f\"Relaxed matching (T1,T2,T3,T5): {results_alt['matching_records_dataset1']} hospital matches\")\n",
    "    print(f\"Additional matches found: {results_alt['matching_records_dataset1'] - results['matching_records_dataset1']}\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    \n",
    "    # Get the matches DataFrame\n",
    "    matches_df_spark_alt = results_alt['matches']\n",
    "    print(f\"Sample matches from alternative analysis (first 10):\")\n",
    "    matches_df_spark_alt.show(10, truncate=False)\n",
    "    print()\n",
    "    \n",
    "    # Save alternative results\n",
    "    matches_df_spark_alt.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\n",
    "        str(outputs_dir / 'matching_records_alt_temp')\n",
    "    )\n",
    "    csv_file_alt = glob.glob(str(outputs_dir / 'matching_records_alt_temp/*.csv'))[0]\n",
    "    shutil.copy(csv_file_alt, str(outputs_dir / 'matching_records_alt.csv'))\n",
    "    shutil.rmtree(str(outputs_dir / 'matching_records_alt_temp'))\n",
    "    print(\"✓ Alternative results saved to outputs/matching_records_alt.csv\")\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"Error during alternative analysis: {e}\")\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f8c62",
   "metadata": {},
   "source": [
    "### Interpreting the Alternative Analysis\n",
    "\n",
    "The alternative analysis uses only 4 tokens (T1, T2, T3, T5) instead of all 5:\n",
    "- **T1**: FirstName + LastName + Sex + BirthDate\n",
    "- **T2**: FirstName + LastName + PostalCode\n",
    "- **T3**: FirstName + LastName + SocialSecurityNumber\n",
    "- **T4**: ❌ *EXCLUDED* - BirthDate + Sex + PostalCode\n",
    "- **T5**: BirthDate + Sex + SocialSecurityNumber\n",
    "\n",
    "By excluding T4, we're being more lenient about postal code consistency. This might find additional matches where:\n",
    "- Postal codes have typos or formatting differences\n",
    "- People have moved between visits\n",
    "- Data entry errors occurred\n",
    "\n",
    "However, this also increases the risk of false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8deb8",
   "metadata": {},
   "source": [
    "## 6. Understand the Results\n",
    "\n",
    "Let's look at what a match actually means by examining some matched records in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24fa176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample matched record\n",
    "if len(matches_df) > 0:\n",
    "    sample_match = matches_df.iloc[0]\n",
    "    hospital_record_id = sample_match['Hospital_RecordId']\n",
    "    pharmacy_record_id = sample_match['Pharmacy_RecordId']\n",
    "    \n",
    "    # Get the original records\n",
    "    hospital_match = hospital_df[hospital_df['RecordId'] == hospital_record_id]\n",
    "    pharmacy_match = pharmacy_df[pharmacy_df['RecordId'] == pharmacy_record_id]\n",
    "    \n",
    "    if len(hospital_match) == 0 or len(pharmacy_match) == 0:\n",
    "        print(f\"Warning: Could not find matching records in original datasets\")\n",
    "        print(f\"Hospital RecordId {hospital_record_id} found: {len(hospital_match) > 0}\")\n",
    "        print(f\"Pharmacy RecordId {pharmacy_record_id} found: {len(pharmacy_match) > 0}\")\n",
    "    else:\n",
    "        hospital_record = hospital_match.iloc[0]\n",
    "        pharmacy_record = pharmacy_match.iloc[0]\n",
    "        \n",
    "        print(\"Sample Match:\")\n",
    "        print(f\"Hospital Record ID: {hospital_record_id}\")\n",
    "        print(f\"Hospital Patient: {hospital_record['FirstName']} {hospital_record['LastName']}\")\n",
    "        print(f\"DOB: {hospital_record['BirthDate']}, SSN: {hospital_record['SocialSecurityNumber']}\")\n",
    "        print()\n",
    "        print(f\"Pharmacy Record ID: {pharmacy_record_id}\")\n",
    "        print(f\"Pharmacy Patient: {pharmacy_record['FirstName']} {pharmacy_record['LastName']}\")\n",
    "        print(f\"DOB: {pharmacy_record['BirthDate']}, SSN: {pharmacy_record['SocialSecurityNumber']}\")\n",
    "        print()\n",
    "        print(\"✓ All 5 tokens matched, confirming this is the same patient!\")\n",
    "else:\n",
    "    print(\"No matches found. This could happen if:\")\n",
    "    print(\"- Different hashing/encryption keys were used\")\n",
    "    print(\"- Data validation rejected records with invalid attributes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef457335",
   "metadata": {},
   "source": [
    "## 7. Advanced PySpark Transformations (Optional)\n",
    "\n",
    "If PySpark is available, we can perform distributed transformations on the tokenized data for large-scale analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark-based transformations for distributed processing\n",
    "if pyspark_available and spark:\n",
    "    try:\n",
    "        from pyspark.sql.functions import col, count as spark_count\n",
    "        \n",
    "        print(\"Performing distributed token analysis with PySpark...\")\n",
    "        print()\n",
    "        \n",
    "        # Load the tokenized data (all columns as strings to avoid type inference issues)\n",
    "        hospital_tokens_spark = spark.read.csv(\n",
    "            str(outputs_dir / 'hospital_tokens.csv'),\n",
    "            header=True,\n",
    "            inferSchema=False  # Keep all as strings\n",
    "        )\n",
    "        pharmacy_tokens_spark = spark.read.csv(\n",
    "            str(outputs_dir / 'pharmacy_tokens.csv'),\n",
    "            header=True,\n",
    "            inferSchema=False  # Keep all as strings\n",
    "        )\n",
    "        \n",
    "        # Analyze token distribution in hospital dataset\n",
    "        print(\"Hospital Token Distribution:\")\n",
    "        hospital_tokens_spark.groupBy(\"RuleId\").agg(spark_count(\"*\").alias(\"count\")).orderBy(\"RuleId\").show()\n",
    "        print()\n",
    "        \n",
    "        # Analyze token distribution in pharmacy dataset\n",
    "        print(\"Pharmacy Token Distribution:\")\n",
    "        pharmacy_tokens_spark.groupBy(\"RuleId\").agg(spark_count(\"*\").alias(\"count\")).orderBy(\"RuleId\").show()\n",
    "        print()\n",
    "        \n",
    "        # Count unique records\n",
    "        hospital_unique = hospital_tokens_spark.select(\"RecordId\").distinct().count()\n",
    "        pharmacy_unique = pharmacy_tokens_spark.select(\"RecordId\").distinct().count()\n",
    "        print(f\"Unique records - Hospital: {hospital_unique}, Pharmacy: {pharmacy_unique}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Note: Advanced transformations not available - {type(e).__name__}\")\n",
    "        print(\"This is optional and does not affect the core PPRL workflow.\")\n",
    "else:\n",
    "    print(\"PySpark not available for advanced transformations.\")\n",
    "    print(\"Core PPRL analysis completed successfully using pandas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35422623",
   "metadata": {},
   "source": [
    "## 8. Privacy and Security Summary\n",
    "\n",
    "This demonstration shows how OpenToken enables privacy-preserving record linkage:\n",
    "\n",
    "### What was protected:\n",
    "- ✓ Raw patient data (names, SSNs, birthdates) was never shared between organizations\n",
    "- ✓ HMAC-SHA256 hashes cannot be reversed to recover original data\n",
    "- ✓ Encryption key controls who can decrypt and perform linkage\n",
    "\n",
    "### What was shared:\n",
    "- • Encrypted tokens for secure transmission\n",
    "- • Matching statistics showing overlap counts\n",
    "- • Metadata with summary information (not raw data)\n",
    "\n",
    "### Key security principles:\n",
    "1. **Strong Encryption**: AES-256-GCM with random IVs prevents pattern analysis\n",
    "2. **Key Management**: Secure sharing and storage of encryption/hashing keys\n",
    "3. **Deterministic Hashing**: HMAC-SHA256 enables comparison without raw data\n",
    "4. **Access Control**: Only authorized parties can decrypt tokens\n",
    "\n",
    "### PySpark Bridge Benefits:\n",
    "- **Distributed Processing**: Handle large datasets across multiple nodes\n",
    "- **Parallel Decryption**: Efficiently decrypt millions of tokens\n",
    "- **Scalable Analysis**: Perform overlap analysis on enterprise-scale data\n",
    "- **Integration**: Native Spark SQL for additional transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f42e1d",
   "metadata": {},
   "source": [
    "## 9. Customization Examples\n",
    "\n",
    "You can customize this demo by modifying the scripts:\n",
    "\n",
    "### Change dataset size and overlap:\n",
    "Edit `scripts/generate_superhero_datasets.py`:\n",
    "```python\n",
    "num_hospital = 500  # Different size\n",
    "num_pharmacy = 600\n",
    "overlap_percentage = 0.50  # 50% overlap instead of 40%\n",
    "```\n",
    "\n",
    "### Use different encryption keys:\n",
    "Edit `scripts/tokenize_datasets.sh`:\n",
    "```bash\n",
    "HASH_KEY=\"YourCustomHashingKey\"\n",
    "ENCRYPTION_KEY=\"YourCustomEncryptionKey-32\"\n",
    "```\n",
    "\n",
    "**Important**: Both organizations must use the same keys for tokens to match!\n",
    "\n",
    "### Scale with PySpark:\n",
    "For large datasets, ensure PySpark is installed:\n",
    "```bash\n",
    "pip install pyspark opentoken-pyspark\n",
    "```\n",
    "\n",
    "The notebook will automatically use distributed processing if available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a16e4",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "This PPRL demo can be adapted for:\n",
    "- Healthcare: Hospital-to-hospital patient matching\n",
    "- Insurance: Claims linkage across providers\n",
    "- Research: Multi-site study participant matching\n",
    "- Government: Cross-agency identity resolution\n",
    "- Financial Services: Anti-fraud systems\n",
    "\n",
    "### With PySpark Bridge:\n",
    "- Scale to petabyte-level datasets\n",
    "- Distribute tokenization across clusters\n",
    "- Parallel overlap analysis\n",
    "- Real-time record linkage pipelines\n",
    "\n",
    "For more information, see the [README.md](./README.md) in this directory and the [main OpenToken documentation](../../README.md)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
